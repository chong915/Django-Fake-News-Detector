{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Reference` : ##\n",
    "https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>kuala lumpur tourism art culture ministry focu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>kuching sarawak record four new patient invest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>johor baru police open investigation paper spr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>johor baru malaysian love like red red rise bl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>petaling jaya one day least love trump even an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>najib loot rm billion bond epf pas rm billion ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>regret inform kelantan state ministry health t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>due influence majority malaysian face salary p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>oldtown curry noodle restaurant rumor contain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>peace upon please meet return kuala ketil hall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>890 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month                                               Text  Type\n",
       "0    Feb,20  kuala lumpur tourism art culture ministry focu...     0\n",
       "1    Feb,20  kuching sarawak record four new patient invest...     0\n",
       "2    Feb,20  johor baru police open investigation paper spr...     0\n",
       "3    Feb,20  johor baru malaysian love like red red rise bl...     0\n",
       "4    Feb,20  petaling jaya one day least love trump even an...     0\n",
       "..      ...                                                ...   ...\n",
       "885  Nov,20  najib loot rm billion bond epf pas rm billion ...     1\n",
       "886  Nov,20  regret inform kelantan state ministry health t...     1\n",
       "887  Nov,20  due influence majority malaysian face salary p...     1\n",
       "888  Nov,20  oldtown curry noodle restaurant rumor contain ...     1\n",
       "889  Nov,20  peace upon please meet return kuala ketil hall...     1\n",
       "\n",
       "[890 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"data_clean.pkl\")\n",
    "dict = {'Real': 0, 'Fake': 1}\n",
    "df[\"Type\"] = df[\"Type\"].map(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Text\"].values\n",
    "y = df[\"Type\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            w_line = line.split()\n",
    "            curr_word = w_line[0]\n",
    "            word_to_vec_map[curr_word] = np.array(w_line[1:])\n",
    "\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe_path = \"C:\\\\Users\\\\munch\\\\Desktop\\\\NLP Pre-Processing\\\\glove.6B\\\\\"\n",
    "word_to_vec_map = read_glove_vector(GloVe_path + \"glove.6B.50d.txt\")\n",
    "\n",
    "maxLen=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "vocab_len = len(words_to_index) + 1\n",
    "embed_vector_len = 50\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "for word, index in words_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        emb_matrix[index, :] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights=[emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 150\n",
    "X_indices = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "X_indices = pad_sequences(X_indices, maxlen=maxLen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 150)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12159, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "feature_extractor = Sequential()\n",
    "feature_extractor.add(Embedding(input_dim=vocab_len, output_dim=50, input_length=maxLen, weights=[emb_matrix], trainable=False))\n",
    "feature_extractor.add(Flatten())\n",
    "#model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_clf = MultinomialNB(alpha=0.84)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(kernel=\"rbf\", C=9.0, gamma=0.00126)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_clf = LogisticRegression(solver=\"lbfgs\", random_state=42, C=96)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.63, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[-1, 2, 3],\n",
    "                [2, 3, 4]])\n",
    "np.abs(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Users\\munch\\anaconda3\\envs\\nlu\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Training MultinomialNB\n",
      "Training SVC\n",
      "Training DecisionTreeClassifier\n",
      "Training RandomForestClassifier\n",
      "Training LogisticRegression\n",
      "Training AdaBoostClassifier\n",
      "Wall time: 1h 57min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "scores_mnb = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "scores_svm = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "scores_dt = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "scores_rf = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "scores_log_reg = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "scores_ada = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "#scores_hard_voting = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "\n",
    "lookup_clf = {0:scores_mnb, 1:scores_svm, 2:scores_dt, 3:scores_rf, 4:scores_log_reg, 5:scores_ada}\n",
    "\n",
    "\n",
    "param_distributions_mnb = {'alpha': np.linspace(0, 2, 20), 'fit_prior': [True, False]}\n",
    "param_distributions_svm = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(0.01, 10)}\n",
    "param_distributions_dt = {\"max_depth\": uniform(5, 30)}\n",
    "param_distributions_rf = {\"n_estimators\": np.arange(5, 100)}\n",
    "param_distributions_log = {\"C\": uniform(0.01, 100)}\n",
    "param_distributions_ada = {\"learning_rate\": uniform(0, 2)}\n",
    "\n",
    "param_distributions = [param_distributions_mnb, param_distributions_svm, param_distributions_dt, param_distributions_rf, param_distributions_log, param_distributions_ada]\n",
    "\n",
    "#scores_nn = {'train_acc':[], 'test_acc':[], 'train_f1':[], 'test_f1':[], 'train_precision':[], 'test_precision':[], 'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]}\n",
    "\n",
    "for train_index, test_index in kf.split(X_indices, y):\n",
    "    X_train, X_test, y_train, y_test = X_indices[train_index], X_indices[test_index], y[train_index], y[test_index]\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    \n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "    \n",
    "    clf_list = [mnb_clf, svm_clf, dt_clf, rf_clf, log_reg_clf, ada_clf]\n",
    "    \n",
    "    for index, clf in enumerate(clf_list):\n",
    "        print(f\"Training {clf.__class__.__name__}\")\n",
    "        \n",
    "        if index == 0:\n",
    "            X_train_features = np.abs(X_train_features)\n",
    "            X_test_features = np.abs(X_test_features)\n",
    "        elif index == 1:\n",
    "            X_train_features = feature_extractor.predict(X_train)\n",
    "            X_test_features = feature_extractor.predict(X_test)\n",
    "        \n",
    "        rnd_search_cv = RandomizedSearchCV(clf, param_distributions[index], n_iter=5, verbose=0, cv=3)\n",
    "        \n",
    "        rnd_search_cv.fit(X_train_features, y_train)\n",
    "        y_train_pred = rnd_search_cv.predict(X_train_features)\n",
    "        #y_train_pred = [1 if pred>0.5 else 0 for pred in y_train_pred]\n",
    "\n",
    "        y_pred = rnd_search_cv.predict(X_test_features)\n",
    "        #y_pred = [1 if pred>0.5 else 0 for pred in predicted_prob]\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        train_precision = precision_score(y_train, y_train_pred)\n",
    "        test_precision = precision_score(y_test, y_pred)\n",
    "\n",
    "        train_recall = recall_score(y_train, y_train_pred)\n",
    "        test_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "        train_roc_auc_score = roc_auc_score(y_train, y_train_pred)\n",
    "        test_roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        scores_clf = lookup_clf[index]\n",
    "        \n",
    "        scores_clf[\"train_acc\"].append(train_accuracy)\n",
    "        scores_clf[\"test_acc\"].append(test_accuracy)\n",
    "        \n",
    "        scores_clf[\"train_f1\"].append(train_f1)\n",
    "        scores_clf[\"test_f1\"].append(test_f1)\n",
    "        \n",
    "        scores_clf[\"train_precision\"].append(train_precision)\n",
    "        scores_clf[\"test_precision\"].append(test_precision)\n",
    "        \n",
    "        scores_clf[\"train_recall\"].append(train_recall)\n",
    "        scores_clf[\"test_recall\"].append(test_recall)\n",
    "        \n",
    "        scores_clf[\"train_auc\"].append(train_roc_auc_score)\n",
    "        scores_clf[\"test_auc\"].append(test_roc_auc_score)\n",
    "        \n",
    "        '''\n",
    "        scores_clf = scores_nn\n",
    "\n",
    "        scores_clf[\"train_acc\"].append(train_accuracy)\n",
    "        scores_clf[\"test_acc\"].append(test_accuracy)\n",
    "\n",
    "        scores_clf[\"train_f1\"].append(train_f1)\n",
    "        scores_clf[\"test_f1\"].append(test_f1)\n",
    "\n",
    "        scores_clf[\"train_precision\"].append(train_precision)\n",
    "        scores_clf[\"test_precision\"].append(test_precision)\n",
    "\n",
    "        scores_clf[\"train_recall\"].append(train_recall)\n",
    "        scores_clf[\"test_recall\"].append(test_recall)\n",
    "\n",
    "        scores_clf[\"train_auc\"].append(train_roc_auc_score)\n",
    "        scores_clf[\"test_auc\"].append(test_roc_auc_score)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "train_acc: 0.9053682896379526\n",
      "test_acc: 0.9044943820224719\n",
      "train_f1: 0.9052721926101617\n",
      "test_f1: 0.9039830804598593\n",
      "train_precision: 0.8961047450459029\n",
      "test_precision: 0.8966323047129325\n",
      "train_recall: 0.9146464646464647\n",
      "test_recall: 0.9136363636363637\n",
      "train_auc: 0.9054713804713805\n",
      "test_auc: 0.9045959595959596\n",
      "\n",
      "SVC\n",
      "train_acc: 1.0\n",
      "test_acc: 0.8853932584269663\n",
      "train_f1: 1.0\n",
      "test_f1: 0.8602579392099815\n",
      "train_precision: 1.0\n",
      "test_precision: 0.9854056553911205\n",
      "train_recall: 1.0\n",
      "test_recall: 0.7818181818181819\n",
      "train_auc: 1.0\n",
      "test_auc: 0.8842424242424243\n",
      "\n",
      "DecisionTreeClassifier\n",
      "train_acc: 1.0\n",
      "test_acc: 0.9011235955056179\n",
      "train_f1: 1.0\n",
      "test_f1: 0.8999681995973186\n",
      "train_precision: 1.0\n",
      "test_precision: 0.8972138196517079\n",
      "train_recall: 1.0\n",
      "test_recall: 0.9045454545454545\n",
      "train_auc: 1.0\n",
      "test_auc: 0.9011616161616162\n",
      "\n",
      "RandomForestClassifier\n",
      "train_acc: 0.999625468164794\n",
      "test_acc: 0.9247191011235955\n",
      "train_f1: 0.9996204131927219\n",
      "test_f1: 0.9183913100716247\n",
      "train_precision: 1.0\n",
      "test_precision: 0.9707853624463775\n",
      "train_recall: 0.9992424242424243\n",
      "test_recall: 0.875\n",
      "train_auc: 0.9996212121212121\n",
      "test_auc: 0.9241666666666666\n",
      "\n",
      "LogisticRegression\n",
      "train_acc: 1.0\n",
      "test_acc: 0.9516853932584269\n",
      "train_f1: 1.0\n",
      "test_f1: 0.950230985030859\n",
      "train_precision: 1.0\n",
      "test_precision: 0.9654144344520803\n",
      "train_recall: 1.0\n",
      "test_recall: 0.9363636363636364\n",
      "train_auc: 1.0\n",
      "test_auc: 0.9515151515151515\n",
      "\n",
      "AdaBoostClassifier\n",
      "train_acc: 1.0\n",
      "test_acc: 0.9561797752808988\n",
      "train_f1: 1.0\n",
      "test_f1: 0.9542339651520321\n",
      "train_precision: 1.0\n",
      "test_precision: 0.9795603370061693\n",
      "train_recall: 1.0\n",
      "test_recall: 0.9318181818181819\n",
      "train_auc: 1.0\n",
      "test_auc: 0.9559090909090909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "clf_list = [mnb_clf, svm_clf, dt_clf, rf_clf, log_reg_clf, ada_clf]\n",
    "ls = [\"train_acc\", \"test_acc\"]\n",
    "for index, scores_clf in lookup_clf.items():\n",
    "    clf_class = clf_list[index].__class__.__name__\n",
    "    print(f\"{clf_class}\")\n",
    "    for key, item in scores_clf.items():\n",
    "        mean = statistics.mean(item)\n",
    "        print(f\"{key}: {mean}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 1.0\n",
      "test_acc: 0.9910112359550561\n",
      "train_f1: 1.0\n",
      "test_f1: 0.990909090909091\n",
      "train_precision: 1.0\n",
      "test_precision: 0.990909090909091\n",
      "train_recall: 1.0\n",
      "test_recall: 0.990909090909091\n",
      "train_auc: 1.0\n",
      "test_auc: 0.991010101010101\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "for key, item in scores_nn.items():\n",
    "    mean = statistics.mean(item)\n",
    "    print(f\"{key}: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_indices, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25239327549934387, 0.9400749206542969]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_indices = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "model.evaluate(X_test_indices, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_prediction(data, text_list_idx):\n",
    "    data['fake news score'] = 0\n",
    "    text_list_idx = pad_sequences(text_list_idx, maxlen=maxLen, padding=\"post\")\n",
    "    text_preds = model.predict(text_list_idx)\n",
    "    data['fake news score'] = text_preds\n",
    "    pred_fake = np.array(list(map(lambda x: 'fake' if x > 0.5 else 'real', text_preds)))\n",
    "    data['predicted type'] = 0\n",
    "    data['predicted type'] = pred_fake\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indices = tokenizer.texts_to_sequences(X)\n",
    "predicted_df = add_score_prediction(df, df_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>fake news score</th>\n",
       "      <th>predicted type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>kuala lumpur tourism art culture ministry focu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>kuching sarawak record four new patient invest...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.448223</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>johor baru police open investigation paper spr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>johor baru malaysian love like red red rise bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb,20</td>\n",
       "      <td>petaling jaya one day least love trump even an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>najib loot rm billion bond epf pas rm billion ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985940</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>regret inform kelantan state ministry health t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978134</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>due influence majority malaysian face salary p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.989442</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>oldtown curry noodle restaurant rumor contain ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624155</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Nov,20</td>\n",
       "      <td>peace upon please meet return kuala ketil hall...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>890 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month                                               Text  Type  \\\n",
       "0    Feb,20  kuala lumpur tourism art culture ministry focu...     0   \n",
       "1    Feb,20  kuching sarawak record four new patient invest...     0   \n",
       "2    Feb,20  johor baru police open investigation paper spr...     0   \n",
       "3    Feb,20  johor baru malaysian love like red red rise bl...     0   \n",
       "4    Feb,20  petaling jaya one day least love trump even an...     0   \n",
       "..      ...                                                ...   ...   \n",
       "885  Nov,20  najib loot rm billion bond epf pas rm billion ...     1   \n",
       "886  Nov,20  regret inform kelantan state ministry health t...     1   \n",
       "887  Nov,20  due influence majority malaysian face salary p...     1   \n",
       "888  Nov,20  oldtown curry noodle restaurant rumor contain ...     1   \n",
       "889  Nov,20  peace upon please meet return kuala ketil hall...     1   \n",
       "\n",
       "     fake news score predicted type  \n",
       "0           0.001334           real  \n",
       "1           0.448223           real  \n",
       "2           0.002466           real  \n",
       "3           0.001535           real  \n",
       "4           0.031954           real  \n",
       "..               ...            ...  \n",
       "885         0.985940           fake  \n",
       "886         0.978134           fake  \n",
       "887         0.989442           fake  \n",
       "888         0.624155           fake  \n",
       "889         0.997773           fake  \n",
       "\n",
       "[890 rows x 5 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
